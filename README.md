# ğŸ§  Automated Domain-Specific Entity Extraction and Information Structuring System  
### by **Team The Coding Penguins ğŸ§** â€” Deviathon 2025  

A local **AI-powered web intelligence system** that automatically crawls any website, extracts domain-specific entities, and structures them into meaningful insights â€” **without using cloud APIs**.  
It combines **focused web crawling**, **NLP-based entity extraction**, and **semantic ranking**, visualized in an interactive **Streamlit dashboard**.  

---

## ğŸ‘¨â€ğŸ’» Team

| Name | Role | Responsibilities |
|------|------|------------------|
| ğŸ§ **Prakarsh Kaushik** | Team Lead & Backend/NLP | FastAPI backend, NLP extractor, integration |
| ğŸ§ **Om Upadhyay** | Web Crawler Developer | Focused crawler, domain parsing, data collection |
| ğŸ§ **Pratiksha** | NLP & Entity Logic | spaCy tuning, rule-based and semantic extraction |
| ğŸ§ **Priyanshu Gautam** | Frontend Developer | Streamlit UI, API integration, visualization |

---

## âš™ï¸ Tech Stack

- **Language:** Python 3.12  
- **Backend:** FastAPI  
- **Frontend:** Streamlit  
- **NLP Engine:** spaCy + Sentence Transformers (local semantic AI)  
- **Crawler:** Requests + BeautifulSoup + Rule-based traversal  
- **Ranking:** Hybrid model (BM25 + Semantic similarity)  
- **Deployment:** Docker (optional)

---

## ğŸš€ Features

âœ… Intelligent domain crawling with depth and page control  
âœ… AI-based entity extraction using local NLP and embeddings  
âœ… Flexible â€” works with *any* domain and keyword context  
âœ… Hybrid keyword + semantic ranking for accuracy  
âœ… Streamlit dashboard for user-friendly visualization  
âœ… 100% offline â€” no external LLM or cloud dependency  

---

## ğŸ“ Project Structure

```text
The Coding Penguins/
â”‚
â”œâ”€â”€ api/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ crawler/                  # Web crawler (Om)
â”‚   â””â”€â”€ scraper.py
â”‚
â”œâ”€â”€ extractor/                # NLP & semantic extractor
â”‚   â”œâ”€â”€ nlp_pipeline.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ui/                       # Streamlit frontend
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ data/                     # Sample crawled & extracted data
â”‚   â”œâ”€â”€ pages.jsonl
â”‚   â””â”€â”€ entities.json
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
````

---

## ğŸ§  How It Works

1. **Crawl** â†’ The crawler scans the domain and collects HTML/text content up to a defined depth and limit.
2. **Extract** â†’ The NLP pipeline identifies meaningful entities using spaCy, rule-based patterns, and semantic embedding similarity.
3. **Rank** â†’ Hybrid model using **BM25 + Sentence Transformers** ranks relevant entities by confidence.
4. **Visualize** â†’ Streamlit UI displays the extracted entities, scores, and snippets interactively.

---

## ğŸ’» Run Locally

### 1ï¸âƒ£ Clone and Setup

```bash
git clone https://github.com/PrakarshKaushik11/The-Coding-Penguins-Deviathon-2025.git
cd The-Coding-Penguins-Deviathon-2025
python -m venv .venv

# Activate the environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

### 2ï¸âƒ£ Start the Backend (FastAPI)

```bash
set PYTHONPATH=%CD%
python -m uvicorn api.main:app --reload --port 8000
```

Open [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) for Swagger API view.

---

### 3ï¸âƒ£ Start the Frontend (Streamlit)

In a new terminal window:

```bash
cd "D:\The Coding Penguins"
.\.venv\Scripts\activate
streamlit run ui/app.py
```

Then open [http://localhost:8501](http://localhost:8501).

---

### 4ï¸âƒ£ Try It!

* Enter any domain, e.g. `https://www.gla.ac.in/alumni`
* (Optional) Add keywords like `alumni, graduate, batch of, class of`
* Enable **Semantic AI**
* Click **Crawl & Extract**
* View structured entities with scores and snippets.

---

## ğŸ§° API Endpoints

|  Method  | Endpoint             | Description                             |
| :------: | :------------------- | :-------------------------------------- |
|  **GET** | `/health`            | API health check                        |
| **POST** | `/crawl`             | Crawl domain and return collected pages |
| **POST** | `/crawl-and-extract` | Crawl + NLP + Semantic extraction       |
|  **GET** | `/results`           | Return extracted entities JSON          |

---

## ğŸ§© Example Workflow

```bash
# Crawl + extract in one step
curl -X POST http://127.0.0.1:8000/crawl-and-extract \
-H "Content-Type: application/json" \
-d "{\"domain\": \"https://www.justice.gov\", \"keywords\": [\"minister\", \"judge\"], \"max_pages\": 20, \"max_depth\": 1}"
```

---

## ğŸ§± Architecture Overview

```
[ Streamlit UI ] 
        â†“
[ FastAPI Backend ]
        â†“
[ Focused Web Crawler ]
        â†“
[ NLP + Semantic Extractor (spaCy + Sentence Transformers) ]
        â†“
[ Ranked Entities & Structured Output ]
```

---

## ğŸ§° Requirements

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

## ğŸ³ Docker (Optional)

```bash
docker-compose up --build
```

* Backend â†’ [http://127.0.0.1:8000](http://127.0.0.1:8000)
* Frontend â†’ [http://localhost:8501](http://localhost:8501)

---

## ğŸ Current Status

âœ… Backend, Crawler, Extractor, and UI integrated
âœ… Semantic ranking implemented using local AI
âœ… 100% offline execution
ğŸš§ Future work â€” Entity linking, summarization, and Docker deployment

---

## ğŸ§© Problem It Solves

Extracting meaningful information from unstructured web data is slow, inconsistent, and domain-dependent.
Our system automates this by combining web crawling with AI-based entity understanding, enabling adaptive extraction for *any* domain â€” efficiently, privately, and offline.

---

## âš™ï¸ Challenges Faced

Building a flexible crawler for diverse site structures, ensuring accurate extraction without noise, optimizing local AI models under limited resources, and integrating backend + UI seamlessly â€” all while maintaining speed and accuracy.

---

## ğŸ“œ License

MIT License Â© 2025 â€” **Team The Coding Penguins ğŸ§**

